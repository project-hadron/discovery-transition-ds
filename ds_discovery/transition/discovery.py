import osfrom builtins import staticmethodfrom contextlib import closingfrom copy import deepcopyfrom pathlib import Pathimport randomfrom typing import Anyimport numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport matplotlib.dates as mdatesimport seaborn as snsimport mathimport timefrom matplotlib.colors import LogNormfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import LabelEncoderfrom ds_discovery.intent.transition_intent import TransitionIntentModel as Intent__author__ = 'Darryl Oatridge'class Visualisation(object):    """ a set of data transition methods to Visualise pandas.Dataframe"""    @staticmethod    def show_num_density(df: pd.DataFrame, category: bool=False, filename: str=None, **kwargs):        """ shows the number density across all the numberic attributes        :param df: the dataframe to visualise        :param category: if True then convert Categoricals to numeric codes and include        :param filename: and optional name of a file to print to        :param kwargs:        """        selection = ['number', 'category'] if category else 'number'        num_cols = Intent.filter_headers(df, dtype=selection)        if len(num_cols) > 0:            depth = int(round(len(num_cols) / 2, 0) + len(num_cols) % 2)            _figsize = (20, 5 * depth)            fig = plt.figure(figsize=_figsize)            right = False            line = 0            for c in num_cols:                col = df[c]                if col.dtype.name == 'category':                    col = col.cat.codes                #     print("{}, {}, {}, {}".format(c, depth, line, right))                ax = plt.subplot2grid((depth, 2), (line, int(right)))                g = col.dropna().plot.kde(ax=ax, title=str.title(c), **kwargs)                g.get_xaxis().tick_bottom()                g.get_yaxis().tick_left()                if right:                    line += 1                right = not right            plt.tight_layout()            if filename is None:                plt.show()            else:                fig.savefig(filename, dpi=300)            plt.clf()        else:            raise LookupError("No numeric columns found in the dataframe")    @staticmethod    def show_cat_count(df, filename=None):        cat_headers = Intent.filter_headers(df, dtype='category')        if len(cat_headers) > 0:            wide_col, thin_col = [], []            for c in cat_headers:                if len(df[c].cat.categories) > 10:                    wide_col += [c]                else:                    thin_col += [c]            depth = len(wide_col) + int(round(len(thin_col) / 2, 0))            _figsize = (20, 5 * depth)            fig = plt.figure(figsize=_figsize)            sns.set(style='darkgrid', color_codes=True)            for c, i in zip(wide_col, range(len(wide_col))):                ax = plt.subplot2grid((depth, 2), (i, 0), colspan=2)                order = list(df[c].value_counts().index.values)                _ = sns.countplot(x=c, data=df, ax=ax, order=order, palette="summer")                _ = plt.xticks(rotation=-90)                _ = plt.xlabel(str.title(c))                _ = plt.ylabel('Count')                title = "{} Categories".format(str.title(c))                _ = plt.title(title, fontsize=16)            right = False            line = len(wide_col)            for c in thin_col:                ax = plt.subplot2grid((depth, 2), (line, int(right)))                order = list(df[c].value_counts().index.values)                _ = sns.countplot(x=c, data=df, ax=ax, order=order, palette="summer")                _ = plt.xticks(rotation=-90)                _ = plt.xlabel(str.title(c))                _ = plt.ylabel('Count')                _ = title = "{} Categories".format(str.title(c))                _ = plt.title(title, fontsize=16)                if right:                    line += 1                right = not right            plt.tight_layout()            if filename is None:                plt.show()            else:                fig.savefig(filename, dpi=300)            plt.clf()        else:            raise LookupError("No category columns found in the dataframe")    @staticmethod    def show_corr(df, filename=None, figsize=None, **kwargs):        if figsize is None or not isinstance(figsize, tuple):            _figsize = (12, 4)        else:            _figsize = figsize        fig = plt.figure(figsize=_figsize)        sns.heatmap(df.corr(), annot=True, cmap='BuGn', robust=True, **kwargs)        if filename is None:            plt.show()        else:            fig.savefig(filename)        plt.clf()    @staticmethod    def show_missing(df, filename=None, figsize=None, **kwargs):        if figsize is None or not isinstance(figsize, tuple):            _figsize = (12, 4)        else:            _figsize = figsize        fig = plt.figure(figsize=_figsize)        sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='viridis', **kwargs)        if filename is None:            plt.show()        else:            fig.savefig(filename)        plt.clf()    @staticmethod    def show_cat_time_index(df, col_index, category=None, col_exclude=None, filename=None, logscale=False,                            subplot_h=2, subplot_w=15, param_scale=8, rotation=360, hspace=0.35):        """ creates the frequencies (colors of heatmap) of the elements (y axis) of the categorical columns        over time (x axis)        :param df: the data frame        :param col_index: the names of the column with the date        :param category: the name of the column to show or the list of columns        :param col_exclude: the name of the column to exclude or the list of columns to exclude        :param filename: output file name        :param logscale: bool, apply log10 transform?        :param subplot_h: the height of the block which corresponds to showing 'param_scale'                        categories on the y axis of the heatmap        :param subplot_w: the width of the figure        :param param_scale: the parameter which controls the height of a single subplot        :param rotation: rotation of the y-axis labels        :param hspace: horizontal space between subplots        """        dates = pd.date_range(start=df[col_index].min(), end=df[col_index].max())        if not isinstance(col_exclude, (np.ndarray, list)):            col_exclude = [col_exclude]        if category is None:            col_names = Intent.filter_headers(df, dtype=['category'], headers=col_exclude, drop=True)        else:            col_names = category            if not isinstance(col_names, (np.ndarray, list)):                col_names = [col_names]        n_categories = len(col_names)        cbar_kws = {'orientation': 'horizontal', 'shrink': 0.5}        n_subplot_rows = np.ceil(df[col_names].nunique(dropna=True).divide(param_scale))        n_subplot_rows[-1] = n_subplot_rows[-1] + 1        n_rows = int(n_subplot_rows.sum())        grid_weights = {'height_ratios': n_subplot_rows.values}        cmap = 'rocket_r'        # cmap = sns.cm.rocket_r        fig, axes = plt.subplots(n_categories, 1, gridspec_kw=grid_weights, sharex='col',                                 figsize=(subplot_w, n_rows * subplot_h))        if n_categories == 1:            axes = [axes]        for ii in range(n_categories):            cc = col_names[ii]            df_single_cat = df[[col_index, cc]]            df_single_cat = df_single_cat.loc[df_single_cat[col_index].notnull(), ]            df_single_cat['Index'] = df_single_cat[col_index].dt.date            df_pivot = df_single_cat.pivot_table(index='Index', columns=cc, aggfunc=len, dropna=True)            df_pivot.index = pd.to_datetime(df_pivot.index)            toplot = df_pivot.reindex(dates.date).T            v_min = toplot.min().min()            v_max = toplot.max().max()            toplot.reset_index(level=0, drop=True, inplace=True)            if logscale:                cbar_ticks = [math.pow(10, i) for i in range(int(math.floor(math.log10(v_min))),                                                             int(1 + math.ceil(math.log10(v_max))))]                log_norm = LogNorm(vmin=v_min, vmax=v_max)            else:                cbar_ticks = list(range(int(v_min), int(v_max + 1)))                if len(cbar_ticks) > 5:                    v_step = int(math.ceil((v_max - v_min) / 4))                    cbar_ticks = list(range(int(v_min), int(v_max + 1), v_step))                log_norm = None            cbar_kws['ticks'] = cbar_ticks            if ii < (n_categories - 1):                cbar_kws['pad'] = 0.05            else:                cbar_kws['pad'] = 0.25            sns.heatmap(toplot, cmap=cmap, ax=axes[ii], norm=log_norm, cbar_kws=cbar_kws, yticklabels=True)            axes[ii].set_ylabel('')            axes[ii].set_xlabel('')            axes[ii].set_title(cc)            axes[ii].set_yticklabels(axes[ii].get_yticklabels(), rotation=rotation)            for _, spine in axes[ii].spines.items():                spine.set_visible(True)        axes[-1].set_xlabel(col_index)        plt.subplots_adjust(bottom=0.05, hspace=hspace)        if filename is None:            plt.show()        else:            fig.savefig(filename)        plt.clf()        return    @staticmethod    def show_percent_cat_time_index(df, col_index, category=None, col_exclude=None, filename=None, subplot_h=6,                                    subplot_w=10, rotation=360):        """ creates the proportion (as percentages) (colors of heatmap) of the apearing elements (y axis)        of the categorical columns over time (x axis)        :param df: the data frame        :param col_index: the names of the column with the date        :param category: the name of the column to show or the list of columns        :param col_exclude: the name of the column to exclude or the list of columns to exclude        :param filename: output file name        :param subplot_h: the height of the figure        :param subplot_w: the width of the figure        :param subplot_w: the width of the figure        :param rotation: rotation of the y-axis labels        """        dates = pd.date_range(start=df[col_index].min(), end=df[col_index].max())        if not isinstance(col_exclude, (np.ndarray, list)):            col_exclude = [col_exclude]        if category is None:            col_names = Intent.filter_headers(df, dtype=['category'], headers=col_exclude, drop=True)        else:            col_names = category            if not isinstance(col_names, (np.ndarray, list)):                col_names = [col_names]        cmap = 'rocket_r'        # cmap = sns.cm.rocket_r        df0 = df[col_names + [col_index]]        df0['Index'] = df0[col_index].dt.date        df_unique = df0[col_names].nunique(dropna=True)        df_agg = df0.groupby('Index').nunique(dropna=True).drop('Index', axis=1)        df_frac = df_agg[col_names].divide(df_unique, axis=1)        df_frac.index = pd.to_datetime(df_frac.index)        toplot = df_frac.reindex(dates.date).T        new_labels = df_unique.index.values + '\n(' + df_unique.values.astype(str) + ')'        fig = plt.figure(figsize=(subplot_w, subplot_h))        ax = sns.heatmap(toplot, cmap=cmap, vmin=0, vmax=1, cbar_kws={'shrink': 0.75})        ax.set_yticklabels(new_labels, rotation=rotation)        ax.set_ylabel('')        ax.set_xlabel(col_index)        cbar = ax.collections[0].colorbar        cbar.set_ticks([0.0, 0.25, 0.5, 0.75, 1.0])        cbar.set_ticklabels(['0%', '25%', '50%', '75%', '100%'])        plt.tight_layout()        if filename is None:            plt.show()        else:            fig.savefig(filename)        plt.clf()class DataAnalytics(object):    label: str    associate: str    dtype: str    selection: list    granularity: [int, float, list]    lower: [int, float]    upper: [int, float]    top: int    precision: int    year_first: bool    day_first: bool    data_format: str    weighting_precision: int    exclude_dominant: bool    weight_pattern: list    weight_map: pd.Series    weight_mean: list    weight_std: list    sample_distribution: list    sample_map: pd.Series    dominant_values: list    dominance_weighting: list    dominant_percent: float    dominance_map: pd.Series    nulls_percent: float    sample: int    outlier_percent: float    mean: [int, float]    var: float    skew: float    kurtosis: float    def __init__(self, analysis: dict, label: str=None):        self.label = label if isinstance(label, str) else 'unnamed'        self.dtype = analysis.get('intent', {}).get('dtype', 'object')        self.selection = analysis.get('intent', {}).get('selection', [])        self.granularity = analysis.get('intent', {}).get('granularity', 1)        self.lower = analysis.get('intent', {}).get('lower', 0.0)        self.upper = analysis.get('intent', {}).get('upper', 1.0)        self.top = analysis.get('intent', {}).get('top', None)        self.precision = analysis.get('intent', {}).get('precision', 3)        self.year_first = analysis.get('intent', {}).get('year_first', False)        self.day_first = analysis.get('intent', {}).get('day_first', False)        self.data_format = analysis.get('intent', {}).get('data_format', None)        self.weighting_precision = analysis.get('intent', {}).get('weighting_precision', None)        self.weight_pattern = analysis.get('patterns', {}).get('weight_pattern', [1])        self.weight_map = pd.Series(data=self.weight_pattern, index=self.selection, copy=True)        self.weight_mean = analysis.get('patterns', {}).get('weight_mean', [])        self.weight_std = analysis.get('patterns', {}).get('weight_std', [])        self.sample_distribution = analysis.get('patterns', {}).get('sample_distribution', [0])        self.sample_map = pd.Series(data=self.sample_distribution, index=self.selection, copy=True)        self.dominant_values = analysis.get('patterns', {}).get('dominant_values', [])        self.dominance_weighting = analysis.get('patterns', {}).get('dominance_weighting', [])        self.dominance_map = pd.Series(data=self.dominance_weighting, index=self.dominant_values, copy=True)        self.dominant_percent = analysis.get('patterns', {}).get('dominant_percent', 0)        self.nulls_percent = analysis.get('stats', {}).get('nulls_percent', 0)        self.sample = analysis.get('stats', {}).get('sample', 0)        self.outlier_percent = analysis.get('stats', {}).get('outlier_percent', 0)        self.mean = analysis.get('stats', {}).get('mean', 0)        self.var = analysis.get('stats', {}).get('var', 0)        self.skew = analysis.get('stats', {}).get('skew', 0)        self.kurtosis = analysis.get('stats', {}).get('kurtosis', 0)        class DataDiscovery(object):    """ a set of data transition methods to view manipulate a pandas.Dataframe"""    def __dir__(self):        rtn_list = []        for m in dir(DataDiscovery):            if not m.startswith('_'):                rtn_list.append(m)        return rtn_list    @staticmethod    def analyse_correlated(df: pd.DataFrame, target: [str, list], threshold: float=None,                           inc_category: bool=False, test_size: float=None,                           random_state: int=None, **classifier_kwargs) -> list:        """ identifies groups of highly correlated columns based on the threshold, returning the column names        of the most important of the correlated group to the target.        ref:  Senliol, Baris, et al (2008). 'Fast Correlation Based Filter (FCBF) .        :param df: the Canonical data to drop correlated collumns from        :param target: a target column or list of columns to relate importance to        :param threshold: (optional) threshold correlation between columns. default 0.998        :param inc_category: (optional) if category type columns should be converted to numeric representations        :param test_size: a test percentage split from the df to avoid over-fitting. Default is 0 for no split        :param random_state: a random state should be applied to the test train split. Default is None        :param classifier_kwargs: kwargs to send to the classifier.        :return: if inplace, returns a formatted cleaner contract for this method, else a deep copy Canonical,.        """        # Code block for intent        threshold = threshold if isinstance(threshold, float) and 0 < threshold < 1 else 0.998        df_numbers = Intent.filter_columns(df, dtype=['number'], exclude=False)        if inc_category:            for col in Intent.filter_columns(df, dtype=['category'], exclude=False):                df_numbers[col] = df[col].cat.codes        df_headers = Intent.filter_columns(df, headers=target, drop=True)        df_target = Intent.filter_columns(df, headers=target, drop=False)        if isinstance(test_size, float) and 0 < test_size < 1:            df_headers, _, df_target, _ = train_test_split(df_headers, df_target, test_size=test_size,                                                           random_state=random_state)        # build a dataframe with the correlation between features        corr_features = df_headers.corr()        corr_features = corr_features.abs().unstack()  # absolute value of corr coef        corr_features = corr_features.sort_values(ascending=False)        corr_features = corr_features[corr_features >= threshold]        corr_features = corr_features[corr_features < 1]        corr_features = pd.DataFrame(corr_features).reset_index()        corr_features.columns = ['feature1', 'feature2', 'corr']        # find groups of correlated features        grouped_feature_ls = []        correlated_groups = []        for feature in corr_features.feature1.unique():            if feature not in grouped_feature_ls:                # find all features correlated to a single feature                correlated_block = corr_features[corr_features.feature1 == feature]                grouped_feature_ls = grouped_feature_ls + list(                    correlated_block.feature2.unique()) + [feature]                # append the block of features to the list                correlated_groups.append(correlated_block)        # use random forest classifier to select features of importance        n_estimators = classifier_kwargs.pop('n_estimators', 200)        random_state = classifier_kwargs.pop('random_state', random_state)        max_depth = classifier_kwargs.pop('max_depth', 4)        rf = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state, max_depth=max_depth,                                    **classifier_kwargs)        importance_set = set()        for group in correlated_groups:            features = list(group.feature2.unique()) + list(group.feature1.unique())            _ = rf.fit(df_headers[features].fillna(0), df_target)            importance = pd.concat([pd.Series(features), pd.Series(rf.feature_importances_)], axis=1)            importance_set.add(importance.iloc[0, 0])        return list(importance_set)    @staticmethod    def analyse_category(categories: Any, lower: [int, float]=None, upper: [int, float]=None, top: int=None,                         nulls_list: list=None, replace_zero: [int, float]=None, weighting_precision: int=None):        """Analyses a set of categories and returns a dictionary of intent and patterns and statitics.        :param categories: the categories to analyse        :param lower: outliers lower limit and below to remove. (optional)                         int represents the category count, (removed before the weighting pattern)                         float between 0 and 1 represents normalised value (removed from weighting pattern)        :param upper: outliers upper limit and above to remove. (optional                         integer represents category count, (removed before the weighting pattern)                         float between 0 and 1 represents normalised value (removed from weighting pattern)        :param top: (optional) only select the top n from a selection, regardless of weighting equivalence.        :param replace_zero: (optional) if zero what to replace the weighting value with to avoid zero probability        :param nulls_list: (optional) a list of nulls if more than the default empty string        :param weighting_precision: (optional) The precision of the weighting values. by default set to 2.        :return: a dictionary of results        """        categories = pd.Series(categories)        weighting_precision = 2 if not isinstance(weighting_precision, int) else weighting_precision        nulls_list = [''] if not isinstance(nulls_list, list) else nulls_list        replace_zero = 0 if not isinstance(replace_zero, (int, float)) else replace_zero        lower = None if not isinstance(lower, (int, float)) else lower        upper = None if not isinstance(upper, (int, float)) else upper        _original_size = categories.size        categories = categories.replace(nulls_list, np.nan).dropna()        nulls_percent = np.round(((_original_size - categories.size) / _original_size) * 100,                                 weighting_precision) if _original_size > 0 else 0        _categories_size = categories.size        value_count = categories.value_counts(sort=True, normalize=False, dropna=True)        # if integer filter out the value counts        if isinstance(lower, int):            value_count = value_count[value_count >= lower]        if isinstance(upper, int):            value_count = value_count[value_count <= upper]        categories = categories[categories.isin(value_count.index)]        value_count = categories.value_counts(sort=True, normalize=True, dropna=True)        # if integer filter out the value counts        if isinstance(lower, float) and 0 <= lower <= 1:            value_count = value_count[value_count >= lower]        if isinstance(upper, float) and 0 <= upper <= 1:            value_count = value_count[value_count <= upper]        if isinstance(top, int) and top > 0:            value_count = value_count.iloc[:top]        categories = categories[categories.isin(value_count.index)]        _outlier_percent = np.round(((_categories_size - categories.size) / _categories_size) * 100,                                    weighting_precision) if _categories_size > 0 else 0        _sample_dist = categories.value_counts(sort=True, normalize=False, dropna=True).to_list()        value_count = value_count.replace(np.nan, 0).replace(0, replace_zero)*100        _weighting = value_count.round(weighting_precision).to_list()        if len(_weighting) == 0:            _weighting = [0]        _lower = lower if isinstance(lower, (int, float)) else _weighting[-1]        _upper = upper if isinstance(upper, (int, float)) else _weighting[0]        rtn_dict = {'intent': {'selection': value_count.index.to_list(), 'dtype': 'category', 'upper': _upper,                               'lower': _lower, 'granularity': value_count.nunique()},                    'patterns': {'weight_pattern': _weighting, 'sample_distribution': _sample_dist},                    'stats': {'nulls_percent': nulls_percent, 'outlier_percent': _outlier_percent,                              'sample': categories.size}}        if isinstance(top, int):            rtn_dict.get('intent')['top'] = top        if isinstance(weighting_precision, int):            rtn_dict.get('intent')['weighting_precision'] = weighting_precision        return rtn_dict    @staticmethod    def analyse_number(values: Any, granularity: [int, float, list]=None, lower: [int, float]=None,                       upper: [int, float]=None, precision: int=None, weighting_precision: int=None,                       dominant: [int, float, list]=None, exclude_dominant: bool=None):        """Analyses a set of values and returns a dictionary of analytical statistics. Unless zero is not common,        to avoid zero values skewing the weighting you should always consider 0 as a dominant value        :param values: the values to analyse        :param granularity: (optional) the granularity of the analysis across the range. Default is 1                int passed - represents the number of periods                float passed - the length of each interval                list[tuple] - specific interval periods e.g []                list[float] - the percentile or quantities, All should fall between 0 and 1        :param lower: (optional) the lower limit of the number value. Default min()        :param upper: (optional) the upper limit of the number value. Default max()        :param precision: (optional) The precision of the range and boundary values. by default set to 3.        :param weighting_precision: (optional) The precision of the weighting values. by default set to 2.        :param dominant: (optional) identify dominant value or list of values, can be empty if None, mode is dominant        :param exclude_dominant: (optional) if the dominant values should be excluded from the weighting. Default False        :return: a dictionary of results        """        values = pd.Series(values)        _original_size = values.size        precision = 3 if not isinstance(precision, int) else precision        weighting_precision = 2 if not isinstance(weighting_precision, int) else weighting_precision        granularity = 1 if not isinstance(granularity, (int, float, list)) or granularity == 0 else granularity        _intervals = granularity        dominant = values.mode(dropna=True).to_list()[:10] if not isinstance(dominant, (int, float, list)) else dominant        dominant = DataDiscovery.list_formatter(dominant)        exclude_dominant = False if not isinstance(exclude_dominant, bool) else exclude_dominant        # limits        if values.size > 0:            lower = values.min() if not isinstance(lower, (int, float)) else lower            upper = values.max() if not isinstance(upper, (int, float)) else upper            if lower >= upper:                upper = lower                _intervals = [(lower, upper, 'both')]        else:            lower = 0            upper = 0            _intervals = [(lower, upper, 'both')]        # nulls        _values_size = values.size        values = values.dropna()        _nulls_size = _original_size - values.size        _nulls_percent = np.round(((_values_size - values.size) / _values_size) * 100,                                  weighting_precision) if _values_size > 0 else 0        # outliers        _values_size = values.size        values = values.loc[values.between(lower, upper, inclusive=True).values]        _outliers_size = _original_size - _nulls_size - _values_size        _outlier_percent = np.round(((_values_size - values.size) / _values_size) * 100,                                    weighting_precision) if _values_size > 0 else 0        # dominance        _dominant_values = values[values.isin(dominant)]        _dominant_count = _dominant_values.value_counts(normalize=False, dropna=True)        _dominance_weighting = _dominant_values.value_counts(normalize=True, dropna=True).round(weighting_precision)*100        _dominance_weighting = [0] if _dominance_weighting.size == 0 else _dominance_weighting.to_list()        _values_size = values.size        if exclude_dominant:            values = values[~values.isin(dominant)]        _dominant_size = _dominant_count.sum()        if _dominant_size == _values_size:            _dominant_percent = 1        else:            _dominant_percent = np.round((_dominant_size / _values_size) * 100,                                         weighting_precision) if _values_size > 0 else 0        # if there are no samples remaining        if values.size == 0:            return {'intent': {'selection': _intervals, 'granularity': granularity, 'precision': precision,                               'lower': np.round(lower, precision), 'upper': np.round(upper, precision),                               'dtype': 'number', 'weighting_precision': weighting_precision},                    'patterns': {'weight_pattern': [1], 'weight_mean': [], 'weight_std': [], 'sample_distribution': [0],                                 'dominant_values': [], 'dominance_weighting': [], 'dominant_percent': 100},                    'stats': {'nulls_percent': _nulls_percent, 'sample': 0, 'outlier_percent': _outlier_percent,                              'mean': 0, 'var': 0, 'skew': 0, 'kurtosis': 0}}        # granularity        if isinstance(_intervals, (int, float)):            # if granularity float then convert frequency to intervals            if isinstance(_intervals, float):                # make sure frequency goes beyond the upper                _end = upper + _intervals - (upper % _intervals)                periods = pd.interval_range(start=lower, end=_end, freq=_intervals).drop_duplicates()                periods = periods.to_tuples().to_list()                _intervals = []                while len(periods) > 0:                    period = periods.pop(0)                    if len(periods) == 0:                        _intervals += [(period[0], period[1], 'both')]                    else:                        _intervals += [(period[0], period[1], 'left')]            # if granularity int then convert periods to intervals            else:                periods = pd.interval_range(start=lower, end=upper, periods=_intervals).drop_duplicates()                _intervals = periods.to_tuples().to_list()        if isinstance(_intervals, list):            if all(isinstance(value, tuple) for value in _intervals):                if len(_intervals[0]) == 2:                    _intervals[0] = (_intervals[0][0], _intervals[0][1], 'both')                _intervals = [(t[0], t[1], 'right') if len(t) == 2 else t for t in _intervals]            elif all(isinstance(value, float) and 0 < value < 1 for value in _intervals):                quantiles = list(set(_intervals + [0, 1.0]))                boundaries = values.quantile(quantiles).values                boundaries.sort()                _intervals = [(boundaries[0], boundaries[1], 'both')]                _intervals += [(boundaries[i - 1], boundaries[i], 'right') for i in range(2, boundaries.size)]            else:                _intervals = (lower, upper, 'both')        # interval weighting        _sample_dist = []        _values_weights = []        _mean_weights = []        _var_weights = []        for interval in _intervals:            low, high, closed = interval            if str.lower(closed) == 'neither':                interval_values = values.loc[(values > low) & (values < high)]            elif str.lower(closed) == 'left':                interval_values = values.loc[(values >= low) & (values < high)]            elif str.lower(closed) == 'both':                interval_values = values.loc[(values >= low) & (values <= high)]            else:  # default right                interval_values = values.loc[(values > low) & (values <= high)]            _values_weights.append(interval_values.size)            _sample_dist.append(interval_values.size)            if interval_values.size == 0:                _mean_weights.append(0.0)                _var_weights.append(0.0)            else:                _mean_weights.append(np.round(interval_values.mean(), weighting_precision + 1))                _var_weights.append(np.round(interval_values.var(), weighting_precision + 1))        if len(_values_weights) == 0:            _values_weights = [0]        _values_weights = pd.Series(_values_weights)        if values.size > 0:            _values_weights = _values_weights.apply(lambda x: np.round((x / values.size) * 100, weighting_precision))        # statistics        _mean = values.mean().round(weighting_precision)        _var = values.var().round(weighting_precision)        _skew = round(values.skew(), weighting_precision)        _kurtosis = round(values.kurtosis(), weighting_precision)        _intervals = [(np.round(p[0], precision), np.round(p[1], precision), p[2]) for p in _intervals]        return {'intent': {'selection': _intervals, 'granularity': granularity, 'dtype': 'number',                           'precision': precision, 'lower': np.round(lower, precision),                           'upper': np.round(upper, precision), 'weighting_precision': weighting_precision},                'patterns': {'weight_pattern': _values_weights.to_list(), 'weight_mean': _mean_weights,                             'weight_std': _var_weights, 'sample_distribution': _sample_dist,                             'dominant_values': dominant, 'dominance_weighting': _dominance_weighting,                             'dominant_percent': _dominant_percent},                'stats': {'nulls_percent': _nulls_percent, 'sample': values.size, 'outlier_percent': _outlier_percent,                          'mean': _mean, 'var': _var, 'skew': _skew, 'kurtosis': _kurtosis}}    @staticmethod    def analyse_date(values: Any, granularity: [int, float, pd.Timedelta]=None, lower: Any=None, upper: Any=None,                     day_first: bool=None, year_first: bool=None, date_format: str=None,                     weighting_precision: int=None):        """Analyses a set of dates and returns a dictionary of selection and weighting        :param values: the values to analyse        :param granularity: (optional) the granularity of the analysis across the range.                int passed - the number of sections to break the value range into                pd.Timedelta passed - a frequency time delta        :param lower: (optional) the lower limit of the number value. Takes min() if not set        :param upper: (optional) the upper limit of the number value. Takes max() if not set        :param day_first: if the date provided has day first        :param year_first: if the date provided has year first        :param date_format: the format of the output dates, if None then pd.Timestamp        :param weighting_precision: (optional) The precision of the weighting values. by default set to 2.        :return: a dictionary of results        """        values = pd.to_datetime(values, errors='coerce', infer_datetime_format=True, dayfirst=day_first,                                yearfirst=year_first)        values = mdates.date2num(values)        values = pd.Series(values)        lower = pd.to_datetime(lower, errors='coerce', infer_datetime_format=True, dayfirst=day_first,                               yearfirst=year_first)        upper = pd.to_datetime(upper, errors='coerce', infer_datetime_format=True, dayfirst=day_first,                               yearfirst=year_first)        lower = values.min() if not isinstance(lower, pd.Timestamp) else mdates.date2num(lower)        upper = values.max() if not isinstance(upper, pd.Timestamp) else mdates.date2num(upper)        if isinstance(granularity, pd.Timedelta):            granularity = mdates.date2num(mdates.num2date(lower) + granularity) - lower        rtn_dict = DataDiscovery.analyse_number(values, granularity=granularity, lower=lower, upper=upper,                                                precision=10, weighting_precision=weighting_precision)        # add the specific data        rtn_dict.get('intent')['day_first'] = False if not isinstance(day_first, bool) else day_first        rtn_dict.get('intent')['year_first'] = False if not isinstance(year_first, bool) else year_first        if isinstance(date_format, str):            rtn_dict.get('intent')['date_format'] = date_format        if isinstance(year_first, bool):            rtn_dict.get('intent')['year_first'] = year_first        if isinstance(day_first, bool):            rtn_dict.get('intent')['day_first'] = day_first        # tidy back all the dates        rtn_dict.get('intent')['selection'] = [(pd.Timestamp(mdates.num2date(p[0])),                                                pd.Timestamp(mdates.num2date(p[1])),                                                p[2]) for p in rtn_dict.get('intent')['selection']]        rtn_dict.get('intent')['lower'] = pd.Timestamp(mdates.num2date(rtn_dict.get('intent')['lower']))        rtn_dict.get('intent')['upper'] = pd.Timestamp(mdates.num2date(rtn_dict.get('intent')['upper']))        rtn_dict.get('stats')['mean'] = pd.Timestamp(mdates.num2date(rtn_dict.get('stats')['mean']))        if isinstance(date_format, str):            rtn_dict.get('intent')['selection'] = [(p[0].strftime(date_format), p[1].strftime(date_format),                                                    p[2]) for p in rtn_dict.get('intent')['selection']]            rtn_dict.get('intent')['lower'] = rtn_dict.get('intent')['lower'].strftime(date_format)            rtn_dict.get('intent')['upper'] = rtn_dict.get('intent')['upper'].strftime(date_format)            rtn_dict.get('stats')['mean'] = rtn_dict.get('stats')['mean'].strftime(date_format)        rtn_dict.get('intent')['dtype'] = 'date'        # remove things that don't make sense to dates        if rtn_dict.get('intent').get('precision') is not None:            rtn_dict.get('intent').pop('precision')        for label in ['dominant_values', 'dominance_weighting', 'dominant_percent', 'weight_std', 'weight_mean']:            if rtn_dict.get('patterns').get(label) is not None:                rtn_dict.get('patterns').pop(label)        if rtn_dict.get('stats').get('var') is not None:            rtn_dict.get('stats').pop('var')        return rtn_dict    @staticmethod    def analyse_association(df: pd.DataFrame, columns_list: list, exclude_associate: list=None):        """ Analyses the association of Category against Values and returns a dictionary of resulting weighting        the structure of the columns_list is a list of dictionaries with the key words            - label: the label or name of the header in the DataFrame            - dtype: one of category|number|date indicating the origin of the data            - chunk_size: if the weighting pattern is over the size of the data the number of chunks            - replace_zero: if a zero reference is returned it can optionally be replaced with a low probability        and example structure might look like:            [{'label1': {'dtype': 'number', 'granularity': 5}},             {'label2': {'dtype': 'category', 'top': 10, 'replace_zero': 0.001}}]        :param df: the DataFrame to take the columns from        :param columns_list: a dictionary structure of columns to select for association        :param exclude_associate: (optional) a list of dot separated tree of items to exclude from iteration (e.g. [age.        :return: a dictionary of association weighting        """        tools = DataDiscovery        if not isinstance(columns_list, list):            raise ValueError("The columns list must be a list of dictionaries")        def _get_weights(_df: pd.DataFrame, columns: list, index: int, weighting: dict, parent: list):            for label, kwargs in columns[index].items():                tree = parent.copy()                tree.append(label)                if '.'.join(tree) in exclude_associate:                    continue                section = {'associate': str('.'.join(tree))}                if label not in _df.columns:                    raise ValueError("header '{}' not found in the DataFrame".format(label))                dtype = kwargs.get('dtype')                lower = kwargs.get('lower')                upper = kwargs.get('upper')                granularity = kwargs.get('granularity')                weighting_precision = kwargs.get('weighting_precision')                if (dtype is None and df[label].dtype in [int, float]) or str(dtype).lower().startswith('number'):                    precision = kwargs.get('precision')                    dominant = kwargs.get('dominant')                    exclude_dominant = kwargs.get('exclude_dominant')                    section['analysis'] = tools.analyse_number(_df[label], granularity=granularity, lower=lower,                                                               upper=upper, precision=precision,                                                               weighting_precision=weighting_precision,                                                               dominant=dominant, exclude_dominant=exclude_dominant)                elif str(dtype).lower().startswith('date'):                    day_first = kwargs.get('day_first')                    year_first = kwargs.get('year_first')                    date_format = kwargs.get('date_format')                    section['analysis'] = tools.analyse_date(_df[label], granularity=granularity, lower=lower,                                                             upper=upper, day_first=day_first, year_first=year_first,                                                             weighting_precision=weighting_precision,                                                             date_format=date_format)                else:                    top = kwargs.get('top')                    replace_zero = kwargs.get('replace_zero')                    nulls_list = kwargs.get('nulls_list')                    section['analysis'] = tools.analyse_category(_df[label], lower=lower, upper=upper, top=top,                                                                 replace_zero=replace_zero, nulls_list=nulls_list,                                                                 weighting_precision=weighting_precision)                # iterate the sub categories                for category in section.get('analysis').get('intent').get('selection'):                    if section.get('sub_category') is None:                        section['sub_category'] = {}                    section.get('sub_category').update({category: {}})                    sub_category = section.get('sub_category').get(category)                    if index < len(columns) - 1:                        if isinstance(category, tuple):                            interval = pd.Interval(left=category[0], right=category[1], closed=category[2])                            df_filter = _df.loc[_df[label].apply(lambda x: x in interval)]                        else:                            df_filter = _df[_df[label] == category]                        _get_weights(df_filter, columns=columns, index=index + 1, weighting=sub_category, parent=tree)                    # tidy empty sub categories                    if section.get('sub_category').get(category) == {}:                        section.pop('sub_category')                weighting[label] = section            return        exclude_associate = list() if not isinstance(exclude_associate, list) else exclude_associate        rtn_dict = {}        _get_weights(df, columns=columns_list, index=0, weighting=rtn_dict, parent=list())        return rtn_dict    @staticmethod    def to_sample_num(df, sample_num=10000, is_random=True, file_name=None, sep=None) -> pd.DataFrame:        """ Creates a sample_num of a pandas dataframe.        This is used to reduce the size of large files when investigating and experimenting.        the rows are selected from the start the middle and the end of the file        :param df: The dataframe to sub file        :param sample_num: the positive sample_num of rows to extract. Default to 10000        :param is_random: how to extract the rows. Default to True                True: will select sample_num random values from the df                False: will take sample_num from top, mid and tail of the df        :param file_name: the name of the csv file to save. Default is None                if no file name is provided the file is NOT saved to persistence        :param sep: the csv file separator. Default to ',' [Comma]        :return: pandas.Dataframe        """        if df is None or len(df) < 1:            return pd.DataFrame()        if is_random:            n = len(df)            if n < sample_num:                sample_num = n            index_list = sorted(random.sample(range(n), k=sample_num))            df_sub = df.iloc[index_list]        else:            diff = sample_num % 3            sample_num = int(sample_num / 3)            df_sub = df.iloc[:sample_num]            mid = int(len(df) / 2)            df_sub = df_sub.append(df.iloc[mid:mid + sample_num + diff])            df_sub = df_sub.append(df.iloc[-sample_num:])        if file_name is not None:            if sep is None:                sep = ','            df_sub.to_csv(file_name, sep=sep, encoding='utf-8')        return df_sub    @staticmethod    def find_file(find_name=None, root_dir=None, ignorecase=True, extensions=None):        """ find file(s) under the root path with the extension types given        find_name can be full or part and will return a pandas.DatafFrame of        matching names with the following headings:        ["name", "parent", "stem", "suffix", "created", "search"]        :param find_name: the name of the item to find. Defualt to None        :param root_dir: the root directory to seach from. Default is cwd        :param ignorecase: if the search should ignore the name case. Default to True        :param extensions: a list of extensions to look for (should start with a .)            Default are ['csv', 'xlsx', 'json', 'p', 'yaml', 'tsv']        :return:            a pandas.DataFrame of files found that match the find_name        """        pd.set_option('max_colwidth', 80)        if root_dir is None:            root_dir = os.getcwd()        if not os.path.exists(root_dir):            raise ValueError('The root path {} does not exist'.format(root_dir))        if extensions is None or not extensions:            extensions = ['csv', 'tsv', 'txt', 'xlsx', 'json', 'pickle', 'p', 'yaml']        extensions = DataDiscovery.list_formatter(extensions)        all_files = []        # Get all the files in the whole directory tree and create the dataframe        for i in Path(root_dir).rglob('*'):            if i.is_file() and i.suffix[1:] in extensions:                all_files.append((i.name, i.parent, i.stem, i.suffix[1:],                                  time.ctime(i.stat().st_ctime), i.name.lower()))        columns = ["name", "parent", "stem", "suffix", "created", "search"]        pd.set_option('display.max_colwidth', -1)        df = pd.DataFrame.from_records(all_files, columns=columns)        if find_name is None:            return df        if ignorecase is True:            return df[df['search'].str.contains(find_name.lower())]        return df[df['name'].str.contains(find_name)]    # @staticmethod    # def massive_data_sampler(filename, chunk_sample=100, sample_limit=0, chunk_size=100000, **kwargs) -> pd.DataFrame:    #     """ takes a sample set of data from massive files that can't be loaded into memory    #    #     :param filename: the name of the csv file to be loaded    #     :param chunk_sample: the sample size from each chunk, if 0 then same as chunk_size    #     :param sample_limit: the the sample limit. if 0 then no limit    #     :param chunk_size: the size of the chunks to take from the file    #     :param kwargs: Additional kwargs to be added to the csv read such as 'sep='    #     :return: the sample pd.DataFrame    #     """    #     if chunk_size is None or chunk_size < 1:    #         chunk_size = 100000    #     if chunk_sample is None or chunk_sample == 0 or chunk_size < chunk_sample:    #         chunk_sample = chunk_size    #     df = None    #     _row_count = 0    #     for chunk in pd.read_csv(filename, chunksize=chunk_size, **kwargs):    #         if chunk.index.max() <= chunk.index.min():    #             break    #         _sample_length = chunk_size - chunk_sample    #         _index_length = chunk.index.max() - chunk.index.min()    #         if _sample_length > _index_length:    #             _sample_length = _index_length    #         _index_list = sorted(random.sample(range(chunk.index.min(), chunk.index.max()), k=_sample_length))    #         chunk_drop = chunk.drop(axis=0, index=_index_list)    #         if df is None:    #             df = chunk_drop    #         else:    #             df = df.append(chunk_drop)    #         _row_count += chunk_sample    #         if _row_count >= sample_limit > 0:    #             break    #     return df    @staticmethod    def train_test_sampler(df, train_fraction=None, random_state=None) -> list:        """ returns a training and testing pandas.Dataframe from the passed one        :param df: the pandas.DataFrame to take the sampler from        :param train_fraction: the percentage fraction of the train model. Default 0.75        :param random_state: a random state value. Default 0 < n < 100        :return: a train df and a test df        """        if train_fraction is None:            train_fraction = 0.75        if random_state is None:            random_state = np.random.randint(1, 100)        train = df.sample(frac=train_fraction, random_state=random_state)        test = df.loc[~df.index.isin(train.index), :]        return [train, test]    # @staticmethod    # def create_dictionary(df, contract_name, file_out) -> None:    #     """ creates an excel spreadsheet of the data dictionary    #    #     :param df: the DataFrame to base the dictionary on    #     :param contract_name: the contract name under which the configuration is stored    #     :param file_out: the name of the excel file    #     :return:    #     """    #     path = file_out    #     with closing(pd.ExcelWriter(path, engine='xlsxwriter')) as writer:    #         # data dictionary    #         sheet = contract_name    #         df_dictionary = DataDiscovery.data_dictionary(df, stylise=False)    #         df_dictionary.to_excel(writer, sheet_name=sheet, startrow=1, index=False, header=False)    #         DataDiscovery._dictionary_format(df_dictionary, writer, sheet)    #         # stat disctionary    #         sheet = '{}_stat'.format(contract_name)    #         df_stats = DataDiscovery.stat_dictionary(df).reset_index().rename(columns={'index': ''})    #         df_stats.to_excel(writer, sheet_name=sheet, startrow=1, index=False, header=False)    #         DataDiscovery._dictionary_format(df_stats, writer, sheet)    #         # category dictionary    #         sheet = '{}_cat'.format(contract_name)    #         col = 0    #         for df_category in DataDiscovery._category_dictionary(df):    #             df_category.to_excel(writer, sheet_name=sheet, startcol=col, index=False, header=True)    #             col += 3    #             DataDiscovery._dictionary_format(None, writer, sheet)    #         writer.save()    #     return    # @staticmethod    # def report_analysis(df, associate: list=None, stylise: bool=False):    #     """    #    #     :param df:    #     :param associate:    #     :return:    #     """    #     stylise = True if not isinstance(stylise, bool) else stylise    #     style = [{'selector': 'th', 'props': [('font-size', "120%"), ("text-align", "center")]},    #              {'selector': '.row_heading, .blank', 'props': [('display', 'none;')]}]    #     pd.set_option('max_colwidth', 75)    #     df_len = len(df)    #     file = []    #     labels = ['Attribute', 'dType', '%_Null', '%_Dom', '%_Nxt', 'Count', 'Unique', 'Observations']    #     for c in associate:    #         pass    @staticmethod    def analysis_dictionary(df, granularity: int=None, col_kwargs: dict=None) -> pd.DataFrame:        """ creates an analysis dictionary that shows an overview of the flat distribution and        weighting of the attributes        :param df: the dataframe to analyse        :param granularity: the granularity to be applied across the df attributes        :param col_kwargs: a column kwargs to specify the analysis of specific attributes                 {'attrA': {'granularity': 6}        :return: an analysis data dictionary        """        tools = DataDiscovery        pd.set_option('max_colwidth', 100)        file = []        labels = ['Attribute', 'Type', 'Unique', '% Nulls', 'Sample', 'Selection', 'Weighting',                  'Granularity', 'Lower', 'Upper']        for c in df.columns.sort_values().values:            result = None            kwargs = {}            if isinstance(col_kwargs, dict) and c in col_kwargs.keys():                kwargs = col_kwargs.get(c)            line = [c,                    str(df[c].dtype),                    df[c].nunique()]            if df[c].dtype.name == 'category' or df[c].dtype.name.startswith('bool'):                result = tools.analyse_category(df[c], **kwargs)            elif df[c].dtype.name.startswith('int') or df[c].dtype.name.startswith('float'):                if isinstance(granularity, int) and 'granularity' not in kwargs.keys():                    kwargs['granularity'] = granularity                result = tools.analyse_number(df[c], **kwargs)            elif df[c].dtype.name.startswith('date'):                if isinstance(granularity, int) and 'granularity' not in kwargs.keys():                    kwargs['granularity'] = granularity                result = tools.analyse_date(df[c], **kwargs)            analysis = DataAnalytics(label=c, analysis=result)            line.append(analysis.nulls_percent)            line.append(analysis.sample)            line.append(analysis.selection)            line.append(analysis.weight_pattern)            line.append(analysis.granularity)            line.append(analysis.lower)            line.append(analysis.upper)            file.append(line)        return pd.DataFrame(file, columns=labels)    @staticmethod    def data_dictionary(df, stylise: bool=False, inc_next_dom: bool=False, report_header: str=None,                        condition: str=None):        """ returns a DataFrame of a data dictionary showing 'Attribute', 'Type', '% Nulls', 'Count',        'Unique', 'Observations' where attribute is the column names in the df        Note that the subject_matter, if used, should be in the form:            { subject_ref, { column_name : text_str}}        the subject reference will be the header of the column and the text_str put in next to each attribute row        :param df: (optional) the pandas.DataFrame to get the dictionary from        :param stylise: (optional) returns a stylised dataframe with formatting        :param inc_next_dom: (optional) if to include the next dominate element column        :param report_header: (optional) filter on a header where the condition is true. Condition must exist        :param condition: (optional) the condition to apply to the header. Header must exist. examples:                ' > 0.95', ".str.contains('shed')"        :return: a pandas.Dataframe        """        stylise = True if not isinstance(stylise, bool) else stylise        inc_next_dom = False if not isinstance(inc_next_dom, bool) else inc_next_dom        style = [{'selector': 'th', 'props': [('font-size', "120%"), ("text-align", "center")]},                 {'selector': '.row_heading, .blank', 'props': [('display', 'none;')]}]        pd.set_option('max_colwidth', 75)        df_len = len(df)        file = []        labels = ['Attribute', 'dType', '%_Null', '%_Dom', '%_Nxt', 'Count', 'Unique', 'Observations']        for c in df.columns.sort_values().values:            line = [c,                    str(df[c].dtype),                    round(df[c].replace('', np.nan).isnull().sum() / df_len, 3)]            # Predominant Difference            nulls_list = ['NaN', 'nan', 'null', ' ', '', 'None']            col = deepcopy(df[c])            for item in nulls_list:                col.replace(item, np.nan, inplace=True)            if len(col.dropna()) > 0:                result = (col.astype(str).value_counts() /                          np.float(len(col.astype(str).dropna()))).sort_values(ascending=False).values                line.append(round(result[0], 3))                if len(result) > 1:                    line.append(round(result[1], 3))                else:                    line.append(0)            else:                line.append(0)                line.append(0)            # value count            line.append(col.astype(str).notnull().sum())            # unique            line.append(col.astype(str).nunique())            # Observations            if col.dtype.name == 'category':                line_str = str('|'.join(col.cat.categories.astype(str)))                line.append('{}...'.format(line_str[:100]) if len(line_str) > 100 else line_str)            elif col.dtype.name == 'bool':                line.append(str(' | '.join(col.map({True: 'True', False: 'False'}).unique())))            elif col.dtype.name == 'object':                value_set = col.astype(str).dropna().drop_duplicates()                if len(value_set) > 0:                    sample_num = 3 if len(value_set) >= 3 else len(value_set)                    sample = str(' | '.join(value_set.sample(n=sample_num).values))                else:                    sample = 'Null Values'                line_str = 'Sample: {}'.format(sample)                line.append('{}...'.format(line_str[:100]) if len(line_str) > 100 else line_str)            elif col.dtype.name.startswith('int') \                    or col.dtype.name.startswith('float') \                    or col.dtype.name.startswith('date'):                my_str = 'max=' + str(col.max()) + ' | min=' + str(col.min())                if col.dtype.name.startswith('date'):                    my_str += ' | yr mean= ' + str(round(col.dt.year.mean(), 0)).partition('.')[0]                else:                    my_str += ' | mean=' + str(round(col.mean(), 2))                line.append(my_str)            else:                line.append('')            file.append(line)        df_dd = pd.DataFrame(file, columns=labels)        if isinstance(report_header, str) and report_header in labels and isinstance(condition, str):            str_value = "df_dd['{}']{}".format(report_header, condition)            try:                df_dd = df_dd.where(eval(str_value)).dropna()            except(SyntaxError, ValueError):                pass        if stylise:            df_style = df_dd.style.set_table_styles(style)            _ = df_style.applymap(DataDiscovery._highlight_null_dom, subset=['%_Null', '%_Dom'])            _ = df_style.applymap(lambda x: 'color: white' if x > 0.98 else 'color: black', subset=['%_Null', '%_Dom'])            _ = df_style.applymap(DataDiscovery._highlight_next, subset=['%_Nxt'])            _ = df_style.applymap(lambda x: 'color: white' if x < 0.02 else 'color: black', subset=['%_Nxt'])            _ = df_style.applymap(DataDiscovery._dtype_color, subset=['dType'])            _ = df_style.applymap(DataDiscovery._color_unique, subset=['Unique'])            _ = df_style.applymap(lambda x: 'color: white' if x < 2 else 'color: black', subset=['Unique'])            _ = df_style.format({'%_Null': "{:.1%}", '%_Dom': '{:.1%}', '%_Nxt': '{:.1%}'})            _ = df_style.set_caption('%_Dom: The % most domninant element - %_Nxt: The % next most dominant element')            _ = df_style.set_properties(subset=['Attribute'],  **{'font-weight': 'bold', 'font-size': "120%"})            if not inc_next_dom:                df_style.hide_columns('%_Nxt')                _ = df_style.set_caption('%_Dom: The % most domninant element')            return df_style        if not inc_next_dom:            df_dd.drop('%_Nxt', axis='columns', inplace=True)        return df_dd    @staticmethod    def _category_dictionary(df) -> list:        rtn_list = []        for c in Intent.filter_columns(df, dtype=['category']):            df_stat = df[c].value_counts().reset_index()            df_stat.columns = [c, 'Count']            rtn_list.append(df_stat)        return rtn_list    @staticmethod    def _dictionary_format(df, writer, sheet):        # First set the workbook up        workbook = writer.book        number_fmt = workbook.add_format({'num_format': '#,##0', 'align': 'right', 'valign': 'top'})        decimal_fmt = workbook.add_format({'num_format': '#,##0.00', 'align': 'right', 'valign': 'top'})        percent_fmt = workbook.add_format({'num_format': '0%', 'align': 'right', 'valign': 'top'})        attr_format = workbook.add_format({'bold': True, 'align': 'left', 'text_wrap': True, 'valign': 'top'})        text_fmt = workbook.add_format({'align': 'left', 'text_wrap': True, 'valign': 'top'})        header_format = workbook.add_format({'bold': True, 'text_wrap': True, 'valign': 'top', 'font_size': '12',                                             'fg_color': '#8B0000', 'font_color': '#FFFFFF', 'border': 1})        # The the worksheets        worksheet = writer.sheets[sheet]        worksheet.set_zoom(100)        if sheet.endswith('_stat'):            worksheet.set_column('A:A', 15, attr_format)            letter_count = 0            pre_letter_count = 0            pre_letter = ''            for index in range(1, len(df.columns)):                letter = chr(letter_count + 65)  # A-Z => 65-91                column = df.columns[index]                width, _ = DataDiscovery.col_width(df, column)                fmt = decimal_fmt if df[column].iloc[0].startswith('float') else number_fmt                worksheet.set_column('{pre}{letter}:{pre}{letter}'.format(pre=pre_letter, letter=letter),                                     width + 4, fmt)                letter_count += 1                if letter == 'Z':                    pre_letter = chr(65 + pre_letter_count)                    pre_letter_count += 1                    letter_count = 0        elif sheet.endswith('_cat'):            for letter in range(65, 91, 3):                worksheet.set_column('{}:{}'.format(chr(letter), chr(letter)), 20, text_fmt)                worksheet.set_column('{}:{}'.format(chr(letter + 1), chr(letter + 1)), 8, number_fmt)                worksheet.conditional_format('{}1:{}40'.format(chr(letter + 1), chr(letter + 1)),                                             {'type': '3_color_scale', 'min_color': "#ecf9ec",                                              'mid_color': "#c6ecc6", 'max_color': "#8cd98c"})        else:            worksheet.set_column('A:A', 20, attr_format)            worksheet.set_column('B:B', 12, text_fmt)            worksheet.set_column('C:C', 8, percent_fmt)            worksheet.set_column('D:E', 10, number_fmt)            worksheet.set_column('F:G', 80, text_fmt)        if not sheet.endswith('_cat'):            for col_num, value in enumerate(df.columns.values):                worksheet.write(0, col_num, value, header_format)        return    @staticmethod    def list_formatter(value) -> [list, None]:        """ Useful utility method to convert any type of str, list, tuple or pd.Series into a list"""        if isinstance(value, (int, float, str, pd.Timestamp)):            return [value]        if isinstance(value, (list, tuple, set)):            return list(value)        if isinstance(value, pd.Series):            return value.tolist()        if isinstance(value, dict):            return list(value.items())        return None    @staticmethod    def col_width(df, column, as_value=False) -> tuple:        """ gets the max and min length or values of a column as a (max, min) tuple        :param df: the pandas.DataFrame        :param column: the column to find the max and min for        :param as_value: if the return should be a length or the values. Default is False        :return: returns a tuple with the (max, min) length or values        """        if as_value:            field_length = df[column].astype(str).str.len()            return df.loc[field_length.argmax(), column], df.loc[field_length.argmin(), column]        return df[column].astype(str).str.len().max(), df[column].astype(str).str.len().min()    @staticmethod    def _dtype_color(dtype: str):        """Apply color to types"""        if str(dtype).startswith('cat'):            color = '#208a0f'        elif str(dtype).startswith('int'):            color = '#0f398a'        elif str(dtype).startswith('float'):            color = '#2f0f8a'        elif str(dtype).startswith('date'):            color = '#790f8a'        elif str(dtype).startswith('bool'):            color = '#08488e'        else:            return ''        return 'color: %s' % color    @staticmethod    def _highlight_null_dom(x: str):        x = float(x)        if not isinstance(x, float) or x < 0.65:            return ''        elif x < 0.85:            color = '#ffede5'        elif x < 0.90:            color = '#fdcdb9'        elif x < 0.95:            color = '#fcb499'        elif x < 0.98:            color = '#fc9576'        elif x < 0.99:            color = '#fb7858'        elif x < 0.997:            color = '#f7593f'        else:            color = '#ec382b'        return 'background-color: %s' % color    @staticmethod    def _highlight_next(x: str):        x = float(x)        if not isinstance(x, float):            return ''        elif x < 0.01:            color = '#ec382b'        elif x < 0.02:            color = '#f7593f'        elif x < 0.03:            color = '#fb7858'        elif x < 0.05:            color = '#fc9576'        elif x < 0.08:            color = '#fcb499'        elif x < 0.12:            color = '#fdcdb9'        elif x < 0.18:            color = '#ffede5'        else:            return ''        return 'background-color: %s' % color    @staticmethod    def _color_unique(x: str):        x = int(x)        if not isinstance(x, int):            return ''        elif x < 2:            color = '#ec382b'        elif x < 3:            color = '#a1cbe2'        elif x < 5:            color = '#84cc83'        elif x < 10:            color = '#a4da9e'        elif x < 20:            color = '#c1e6ba'        elif x < 50:            color = '#e5f5e0'        elif x < 100:            color = '#f0f9ed'        else:            return ''        return 'background-color: %s' % color