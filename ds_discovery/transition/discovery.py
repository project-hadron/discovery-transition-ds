import osfrom builtins import staticmethodfrom contextlib import closingfrom copy import deepcopyfrom pathlib import Pathimport randomfrom typing import Anyimport numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport matplotlib.dates as mdatesimport seaborn as snsimport mathimport timefrom matplotlib.colors import LogNormfrom ds_foundation.properties.abstract_properties import AbstractPropertyManagerfrom ds_discovery.cleaners.pandas_cleaners import PandasCleaners as Cleaner__author__ = 'Darryl Oatridge'class Visualisation(object):    """ a set of data transition methods to Visualise pandas.Dataframe"""    @staticmethod    def show_num_density(df: pd.DataFrame, category: bool=False, filename: str=None, **kwargs):        """ shows the number density across all the numberic attributes        :param df: the dataframe to visualise        :param category: if True then convert Categoricals to numeric codes and include        :param filename: and optional name of a file to print to        :param kwargs:        """        selection = ['number', 'category'] if category else 'number'        num_cols = Cleaner.filter_headers(df, dtype=selection)        if len(num_cols) > 0:            depth = int(round(len(num_cols) / 2, 0) + len(num_cols) % 2)            _figsize = (20, 5 * depth)            fig = plt.figure(figsize=_figsize)            right = False            line = 0            for c in num_cols:                col = df[c]                if col.dtype.name == 'category':                    col = col.cat.codes                #     print("{}, {}, {}, {}".format(c, depth, line, right))                ax = plt.subplot2grid((depth, 2), (line, int(right)))                g = col.dropna().plot.kde(ax=ax, title=str.title(c), **kwargs)                g.get_xaxis().tick_bottom()                g.get_yaxis().tick_left()                if right:                    line += 1                right = not right            plt.tight_layout()            if filename is None:                plt.show()            else:                fig.savefig(filename, dpi=300)            plt.clf()        else:            raise LookupError("No numeric columns found in the dataframe")    @staticmethod    def show_cat_count(df, filename=None):        cat_headers = Cleaner.filter_headers(df, dtype='category')        if len(cat_headers) > 0:            wide_col, thin_col = [], []            for c in cat_headers:                if len(df[c].cat.categories) > 10:                    wide_col += [c]                else:                    thin_col += [c]            depth = len(wide_col) + int(round(len(thin_col) / 2, 0))            _figsize = (20, 5 * depth)            fig = plt.figure(figsize=_figsize)            sns.set(style='darkgrid', color_codes=True)            for c, i in zip(wide_col, range(len(wide_col))):                ax = plt.subplot2grid((depth, 2), (i, 0), colspan=2)                order = list(df[c].value_counts().index.values)                _ = sns.countplot(x=c, data=df, ax=ax, order=order, palette="summer")                _ = plt.xticks(rotation=-90)                _ = plt.xlabel(str.title(c))                _ = plt.ylabel('Count')                title = "{} Categories".format(str.title(c))                _ = plt.title(title, fontsize=16)            right = False            line = len(wide_col)            for c in thin_col:                ax = plt.subplot2grid((depth, 2), (line, int(right)))                order = list(df[c].value_counts().index.values)                _ = sns.countplot(x=c, data=df, ax=ax, order=order, palette="summer")                _ = plt.xticks(rotation=-90)                _ = plt.xlabel(str.title(c))                _ = plt.ylabel('Count')                _ = title = "{} Categories".format(str.title(c))                _ = plt.title(title, fontsize=16)                if right:                    line += 1                right = not right            plt.tight_layout()            if filename is None:                plt.show()            else:                fig.savefig(filename, dpi=300)            plt.clf()        else:            raise LookupError("No category columns found in the dataframe")    @staticmethod    def show_corr(df, filename=None, figsize=None, **kwargs):        if figsize is None or not isinstance(figsize, tuple):            _figsize = (12, 4)        else:            _figsize = figsize        fig = plt.figure(figsize=_figsize)        sns.heatmap(df.corr(), annot=True, cmap='BuGn', robust=True, **kwargs)        if filename is None:            plt.show()        else:            fig.savefig(filename)        plt.clf()    @staticmethod    def show_missing(df, filename=None, figsize=None, **kwargs):        if figsize is None or not isinstance(figsize, tuple):            _figsize = (12, 4)        else:            _figsize = figsize        fig = plt.figure(figsize=_figsize)        sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='viridis', **kwargs)        if filename is None:            plt.show()        else:            fig.savefig(filename)        plt.clf()    @staticmethod    def show_cat_time_index(df, col_index, category=None, col_exclude=None, filename=None, logscale=False,                            subplot_h=2, subplot_w=15, param_scale=8, rotation=360, hspace=0.35):        """ creates the frequencies (colors of heatmap) of the elements (y axis) of the categorical columns        over time (x axis)        :param df: the data frame        :param col_index: the names of the column with the date        :param category: the name of the column to show or the list of columns        :param col_exclude: the name of the column to exclude or the list of columns to exclude        :param filename: output file name        :param logscale: bool, apply log10 transform?        :param subplot_h: the height of the block which corresponds to showing 'param_scale'                        categories on the y axis of the heatmap        :param subplot_w: the width of the figure        :param param_scale: the parameter which controls the height of a single subplot        :param rotation: rotation of the y-axis labels        :param hspace: horizontal space between subplots        """        dates = pd.date_range(start=df[col_index].min(), end=df[col_index].max())        if not isinstance(col_exclude, (np.ndarray, list)):            col_exclude = [col_exclude]        if category is None:            col_names = Cleaner.filter_headers(df, dtype=['category'], headers=col_exclude, drop=True)        else:            col_names = category            if not isinstance(col_names, (np.ndarray, list)):                col_names = [col_names]        n_categories = len(col_names)        cbar_kws = {'orientation': 'horizontal', 'shrink': 0.5}        n_subplot_rows = np.ceil(df[col_names].nunique(dropna=True).divide(param_scale))        n_subplot_rows[-1] = n_subplot_rows[-1] + 1        n_rows = int(n_subplot_rows.sum())        grid_weights = {'height_ratios': n_subplot_rows.values}        cmap = 'rocket_r'        # cmap = sns.cm.rocket_r        fig, axes = plt.subplots(n_categories, 1, gridspec_kw=grid_weights, sharex='col',                                 figsize=(subplot_w, n_rows * subplot_h))        if n_categories == 1:            axes = [axes]        for ii in range(n_categories):            cc = col_names[ii]            df_single_cat = df[[col_index, cc]]            df_single_cat = df_single_cat.loc[df_single_cat[col_index].notnull(), ]            df_single_cat['Index'] = df_single_cat[col_index].dt.date            df_pivot = df_single_cat.pivot_table(index='Index', columns=cc, aggfunc=len, dropna=True)            df_pivot.index = pd.to_datetime(df_pivot.index)            toplot = df_pivot.reindex(dates.date).T            v_min = toplot.min().min()            v_max = toplot.max().max()            toplot.reset_index(level=0, drop=True, inplace=True)            if logscale:                cbar_ticks = [math.pow(10, i) for i in range(int(math.floor(math.log10(v_min))),                                                             int(1 + math.ceil(math.log10(v_max))))]                log_norm = LogNorm(vmin=v_min, vmax=v_max)            else:                cbar_ticks = list(range(int(v_min), int(v_max + 1)))                if len(cbar_ticks) > 5:                    v_step = int(math.ceil((v_max - v_min) / 4))                    cbar_ticks = list(range(int(v_min), int(v_max + 1), v_step))                log_norm = None            cbar_kws['ticks'] = cbar_ticks            if ii < (n_categories - 1):                cbar_kws['pad'] = 0.05            else:                cbar_kws['pad'] = 0.25            sns.heatmap(toplot, cmap=cmap, ax=axes[ii], norm=log_norm, cbar_kws=cbar_kws, yticklabels=True)            axes[ii].set_ylabel('')            axes[ii].set_xlabel('')            axes[ii].set_title(cc)            axes[ii].set_yticklabels(axes[ii].get_yticklabels(), rotation=rotation)            for _, spine in axes[ii].spines.items():                spine.set_visible(True)        axes[-1].set_xlabel(col_index)        plt.subplots_adjust(bottom=0.05, hspace=hspace)        if filename is None:            plt.show()        else:            fig.savefig(filename)        plt.clf()        return    @staticmethod    def show_percent_cat_time_index(df, col_index, category=None, col_exclude=None, filename=None, subplot_h=6,                                    subplot_w=10, rotation=360):        """ creates the proportion (as percentages) (colors of heatmap) of the apearing elements (y axis)        of the categorical columns over time (x axis)        :param df: the data frame        :param col_index: the names of the column with the date        :param category: the name of the column to show or the list of columns        :param col_exclude: the name of the column to exclude or the list of columns to exclude        :param filename: output file name        :param subplot_h: the height of the figure        :param subplot_w: the width of the figure        :param subplot_w: the width of the figure        :param rotation: rotation of the y-axis labels        """        dates = pd.date_range(start=df[col_index].min(), end=df[col_index].max())        if not isinstance(col_exclude, (np.ndarray, list)):            col_exclude = [col_exclude]        if category is None:            col_names = Cleaner.filter_headers(df, dtype=['category'], headers=col_exclude, drop=True)        else:            col_names = category            if not isinstance(col_names, (np.ndarray, list)):                col_names = [col_names]        cmap = 'rocket_r'        # cmap = sns.cm.rocket_r        df0 = df[col_names + [col_index]]        df0['Index'] = df0[col_index].dt.date        df_unique = df0[col_names].nunique(dropna=True)        df_agg = df0.groupby('Index').nunique(dropna=True).drop('Index', axis=1)        df_frac = df_agg[col_names].divide(df_unique, axis=1)        df_frac.index = pd.to_datetime(df_frac.index)        toplot = df_frac.reindex(dates.date).T        new_labels = df_unique.index.values + '\n(' + df_unique.values.astype(str) + ')'        fig = plt.figure(figsize=(subplot_w, subplot_h))        ax = sns.heatmap(toplot, cmap=cmap, vmin=0, vmax=1, cbar_kws={'shrink': 0.75})        ax.set_yticklabels(new_labels, rotation=rotation)        ax.set_ylabel('')        ax.set_xlabel(col_index)        cbar = ax.collections[0].colorbar        cbar.set_ticks([0.0, 0.25, 0.5, 0.75, 1.0])        cbar.set_ticklabels(['0%', '25%', '50%', '75%', '100%'])        plt.tight_layout()        if filename is None:            plt.show()        else:            fig.savefig(filename)        plt.clf()class DataDiscovery(object):    """ a set of data transition methods to view manipulate a pandas.Dataframe"""    def __dir__(self):        rtn_list = []        for m in dir(DataDiscovery):            if not m.startswith('_'):                rtn_list.append(m)        return rtn_list    @staticmethod    def analyse_date(values: Any, granularity: [int, float, pd.Timedelta]=None, lower: Any = None, upper: Any = None,                     day_first: bool=False, year_first: bool=False, date_format: str = None, chunk_size: int = None,                     replace_zero: [int, float] = None):        """Analyses a set of dates and returns a dictionary of selection and weighting        :param values: the values to analyse        :param granularity: (optional) the granularity of the analysis across the range.                int passed - the number of sections to break the value range into                pd.Timedelta passed - a frequency time delta        :param lower: (optional) the lower limit of the number value. Takes min() if not set        :param upper: (optional) the upper limit of the number value. Takes max() if not set        :param day_first: if the date provided has day first        :param year_first: if the date provided has year first        :param date_format: the format of the output dates, if None then pd.Timestamp        :param chunk_size: (optional) number of chuncks if you want weighting over the length of the dataset        :param replace_zero: (optional) if zero what to replace the weighting value with to avoid zero probability        :return: a dictionary of results        """        values = pd.to_datetime(values, errors='coerce', infer_datetime_format=True, dayfirst=day_first,                                yearfirst=year_first)        values = mdates.date2num(values)        values = pd.Series(values)        lower = pd.to_datetime(lower, errors='coerce', infer_datetime_format=True, dayfirst=day_first,                               yearfirst=year_first)        upper = pd.to_datetime(upper, errors='coerce', infer_datetime_format=True, dayfirst=day_first,                               yearfirst=year_first)        lower = values.min() if not isinstance(lower, pd.Timestamp) else mdates.date2num(lower)        upper = values.max() if not isinstance(upper, pd.Timestamp) else mdates.date2num(upper)        if lower < upper:            if isinstance(granularity, pd.Timedelta):                granularity = mdates.date2num(mdates.num2date(lower) + granularity) - lower            rtn_dict = DataDiscovery.analyse_number(values, granularity=granularity, lower=lower, upper=upper,                                                    chunk_size=chunk_size, replace_zero=replace_zero, precision=10)        else:            value_size = values.size            values = values.dropna()            null_values = np.round(((value_size - values.size) / value_size) * 100, 2) if value_size > 0 else 0            rtn_dict = {'selection': [(lower, upper)], 'weighting': [100 - null_values], 'lower': lower, 'upper': upper,                        'granularity': granularity, 'dtype': 'date', 'quantity': null_values, 'sample': [values.size],                        'dropped': [0]}        # tidy back all the dates        rtn_dict['selection'] = [(tuple([pd.Timestamp(mdates.num2date(y)) for y in x])) for x in rtn_dict['selection']]        rtn_dict['lower'] = pd.Timestamp(mdates.num2date(rtn_dict['lower']))        rtn_dict['upper'] = pd.Timestamp(mdates.num2date(rtn_dict['upper']))        if isinstance(date_format, str):            rtn_dict['selection'] = [(tuple([y.strftime(date_format) for y in x])) for x in rtn_dict['selection']]            rtn_dict['lower'] = rtn_dict['lower'].strftime(date_format)            rtn_dict['upper'] = rtn_dict['upper'].strftime(date_format)        rtn_dict['dtype'] = 'date'        return rtn_dict    @staticmethod    def analyse_number(values: Any, granularity: [int, float]=None, lower: [int, float]=None, upper: [int, float]=None,                       chunk_size: int=None, replace_zero: [int, float]=None, precision: int=None):        """Analyses a set of values and returns a dictionary of selection and weighting        :param values: the values to analyse        :param granularity: (optional) the granularity of the analysis across the range.                int passed - represents the number of periods                float passed - the length of each interval        :param lower: (optional) the lower limit of the number value. Default min()        :param upper: (optional) the upper limit of the number value. Default max()        :param chunk_size: (optional) number of chuncks if you want weighting over the length of the dataset        :param replace_zero: (optional) if zero what to replace the weighting value with to avoid zero probability        :param precision: (optional) by default set to 3.        :return: a dictionary of results        """        values = pd.Series(values)        precision = 3 if not isinstance(precision, int) else precision        granularity = 3 if not isinstance(granularity, (int, float)) else granularity        replace_zero = 0 if not isinstance(replace_zero, (int, float)) else replace_zero        chunk_size = 1 if not isinstance(chunk_size, int) or chunk_size > values.size else chunk_size        lower = values.min() if not isinstance(lower, (int, float)) else lower        upper = values.max() if not isinstance(upper, (int, float)) else upper        if lower >= upper:            value_size = values.size            values = values.dropna()            null_values = np.round(((value_size - values.size) / value_size) * 100, 2) if value_size > 0 else 0            return {'selection': [(lower, upper)], 'weighting': [100-null_values], 'lower': lower, 'upper': upper,                    'granularity': granularity, 'dtype': 'number', 'null_values': null_values, 'sample': [values.size],                    'dropped': [0]}        # get the intervals so the interval range is fixed across the chunks        freq = granularity if isinstance(granularity, float) else None        periods = granularity if isinstance(granularity, int) else None        # ensure the frequency is equal to or higher than the upper value        _upper = upper + freq - (upper % freq) if periods is None else upper        interval_range = pd.interval_range(start=lower, end=_upper, periods=periods, freq=freq, closed='both')        interval_range = interval_range.drop_duplicates()        weighting = []        null_values = []        sample = []        chunk_drop = []        for chunk in np.array_split(values, chunk_size):            value_size = chunk.size            chunk_nulls = value_size - chunk.dropna().size            # remove nulls and anything not in range            chunk = chunk.loc[chunk.between(lower, _upper, inclusive=True).values]            chunk_drop.append(value_size - (chunk.size + chunk_nulls))            value_size = chunk.size + chunk_nulls            null_values.append(np.round((chunk_nulls / value_size) * 100, 2) if value_size > 0 else 0)            sample.append(chunk.size)            chunk_weights = [0] * len(interval_range)            for v in chunk:                try:                    chunk_weights[interval_range.get_loc(v)[-1]] += 1                except IndexError:                    pass            chunk_weights = pd.Series(chunk_weights)            if value_size > 0:                chunk_weights = chunk_weights.apply(lambda x: np.round((x / value_size) * 100, 3))            weighting.append(chunk_weights.replace(0, replace_zero).tolist())        if len(weighting) == 1:            weighting = weighting[0]        selection = interval_range.to_tuples().tolist()        selection = [(tuple([round(y, precision) if isinstance(y, (int, float)) else y for y in x])) for x in selection]        rtn_dict = {'selection': selection, 'weighting': weighting, 'lower': lower, 'upper': upper,                    'granularity': granularity, 'dtype': 'number', 'null_values': null_values, 'sample': sample,                    'dropped': chunk_drop}        return rtn_dict    @staticmethod    def analyse_category(categories: Any, chunk_size: int=None, replace_zero: [int, float]=None, nulls_list: list=None):        """Analyses a set of categories and returns a dictionary of selection and weighting        the return is in the form:                {'selection': [], 'weighting': []}        :param categories: the categories to analyse        :param chunk_size: (optional) number of chuncks if you want weighting over the length of the dataset        :param replace_zero: (optional) if zero what to replace the weighting value with to avoid zero probability        :param nulls_list: (optional) a list of nulls if more than the default empty string        :return: a dictionary of results        """        categories = pd.Series(categories)        nulls_list = [''] if not isinstance(nulls_list, list) else nulls_list        replace_zero = 0 if not isinstance(replace_zero, (int, float)) else replace_zero        chunk_size = 1 if not isinstance(chunk_size, int) or chunk_size > categories.size else chunk_size        selection = pd.Series(data=0, index=categories.replace(nulls_list, np.nan).dropna().unique())        weighting = []        null_values = []        sample = []        for chunk in np.array_split(categories, chunk_size):            cat_size = chunk.size            chunk = chunk.replace(nulls_list, np.nan).dropna()            null_values.append(np.round(((cat_size - chunk.size) / cat_size) * 100, 2) if cat_size > 0 else 0)            sample.append(chunk.size)            value_count = chunk.value_counts()            if value_count.sum() > 0:                value_count = value_count.apply(lambda x: np.round((x / value_count.sum()) * 100, 3))            value_count = pd.Series(selection.index.map(value_count))            weighting.append(value_count.replace(np.nan, 0).replace(0, replace_zero).tolist())        if len(weighting) == 1:            weighting = weighting[0]        rtn_dict = {'selection': categories.replace(nulls_list, np.nan).dropna().unique().tolist(),                    'weighting': weighting, 'dtype': 'category', 'null_values': null_values, 'sample': sample}        return rtn_dict    @staticmethod    def analyse_association(df: pd.DataFrame, columns_list: list, exclude_associate: list=None):        """ Analyses the association of Category against Values and returns a dictionary of resulting weighting        the structure of the columns_list is a list of dictionaries with the key words            - label: the label or name of the header in the DataFrame            - dtype: one of category|number|date indicating the origin of the data            - chunk_size: if the weighting pattern is over the size of the data the number of chunks            - replace_zero: if a zero reference is returned it can optionally be replaced with a low probability        and example structure might look like:            [{'label1': {'dtype': 'number', 'granularity': 5, 'replace_zero': 0.001}},             {'label2': {'dtype': 'category'}}]        :param df: the dataframe to take the columns from        :param columns_list: a dictionary structure of collumns to select for association        :param exclude_associate: (optional) a list of dot separated tree of items to exclude from iteration (e.g. [age.        :return: a dictionary of association weighting        """        tools = DataDiscovery        if not isinstance(columns_list, list):            raise ValueError("The columns list must be a list of dictionaries")        def _get_weights(_df: pd.DataFrame, columns: list, index: int, weighting: dict, parent: list):            for label, kwargs in columns[index].items():                tree = parent.copy()                tree.append(label)                if '.'.join(tree) in exclude_associate:                    continue                section = {'associate': str('.'.join(tree))}                if label not in _df.columns:                    raise ValueError("header '{}' not found in the Dataframe".format(label))                dtype = kwargs.get('dtype')                chunk_size = kwargs.get('chunk_size')                replace_zero = kwargs.get('replace_zero')                if (dtype is None and df[label].dtype in [int, float]) or str(dtype).lower().startswith('number'):                    granularity = kwargs.get('granularity')                    lower = kwargs.get('lower')                    upper = kwargs.get('upper')                    precision = kwargs.get('precision')                    section['analysis'] = tools.analyse_number(_df[label], granularity=granularity, lower=lower,                                                               upper=upper, chunk_size=chunk_size,                                                               replace_zero=replace_zero, precision=precision)                elif str(dtype).lower().startswith('date'):                    granularity = kwargs.get('granularity')                    lower = kwargs.get('lower')                    upper = kwargs.get('upper')                    day_first = kwargs.get('day_first')                    year_first = kwargs.get('year_first')                    date_format = kwargs.get('date_format')                    section['analysis'] = tools.analyse_date(_df[label], granularity=granularity, lower=lower,                                                             upper=upper, day_first=day_first, year_first=year_first,                                                             chunk_size=chunk_size, replace_zero=replace_zero,                                                             date_format=date_format)                else:                    nulls_list = kwargs.get('nulls_list')                    section['analysis'] = tools.analyse_category(_df[label], chunk_size=chunk_size,                                                                 replace_zero=replace_zero, nulls_list=nulls_list)                for category in section.get('analysis').get('selection'):                    if section.get('sub_category') is None:                        section['sub_category'] = {}                    section.get('sub_category').update({category: {}})                    sub_category = section.get('sub_category').get(category)                    if index < len(columns) - 1:                        if isinstance(category, tuple):                            interval = pd.Interval(left=category[0], right=category[1])                            df_filter = _df.loc[_df[label].apply(lambda x: x in interval)]                        else:                            df_filter = _df[_df[label] == category]                        _get_weights(df_filter, columns=columns, index=index + 1, weighting=sub_category, parent=tree)                    # tidy empty sub categories                    if section.get('sub_category').get(category) == {}:                        section.pop('sub_category')                weighting[label] = section            return        exclude_associate = list() if not isinstance(exclude_associate, list) else exclude_associate        rtn_dict = {}        _get_weights(df, columns=columns_list, index=0, weighting=rtn_dict, parent=list())        return rtn_dict    @staticmethod    def to_sample_num(df, sample_num=10000, is_random=True, file_name=None, sep=None) -> pd.DataFrame:        """ Creates a sample_num of a pandas dataframe.        This is used to reduce the size of large files when investigating and experimenting.        the rows are selected from the start the middle and the end of the file        :param df: The dataframe to sub file        :param sample_num: the positive sample_num of rows to extract. Default to 10000        :param is_random: how to extract the rows. Default to True                True: will select sample_num random values from the df                False: will take sample_num from top, mid and tail of the df        :param file_name: the name of the csv file to save. Default is None                if no file name is provided the file is NOT saved to persistence        :param sep: the csv file separator. Default to ',' [Comma]        :return: pandas.Dataframe        """        if df is None or len(df) < 1:            return pd.DataFrame()        if is_random:            n = len(df)            if n < sample_num:                sample_num = n            index_list = sorted(random.sample(range(n), k=sample_num))            df_sub = df.iloc[index_list]        else:            diff = sample_num % 3            sample_num = int(sample_num / 3)            df_sub = df.iloc[:sample_num]            mid = int(len(df) / 2)            df_sub = df_sub.append(df.iloc[mid:mid + sample_num + diff])            df_sub = df_sub.append(df.iloc[-sample_num:])        if file_name is not None:            if sep is None:                sep = ','            df_sub.to_csv(file_name, sep=sep, encoding='utf-8')        return df_sub    @staticmethod    def find_file(find_name=None, root_dir=None, ignorecase=True, extensions=None):        """ find file(s) under the root path with the extension types given        find_name can be full or part and will return a pandas.DatafFrame of        matching names with the following headings:        ["name", "parent", "stem", "suffix", "created", "search"]        :param find_name: the name of the item to find. Defualt to None        :param root_dir: the root directory to seach from. Default is cwd        :param ignorecase: if the search should ignore the name case. Default to True        :param extensions: a list of extensions to look for (should start with a .)            Default are ['csv', 'xlsx', 'json', 'p', 'yaml', 'tsv']        :return:            a pandas.DataFrame of files found that match the find_name        """        pd.set_option('max_colwidth', 80)        if root_dir is None:            root_dir = os.getcwd()        if not os.path.exists(root_dir):            raise ValueError('The root path {} does not exist'.format(root_dir))        if extensions is None or not extensions:            extensions = ['csv', 'tsv', 'txt', 'xlsx', 'json', 'pickle', 'p', 'yaml']        AbstractPropertyManager.list_formatter(extensions)        all_files = []        # Get all the files in the whole directory tree and create the dataframe        for i in Path(root_dir).rglob('*'):            if i.is_file() and i.suffix[1:] in extensions:                all_files.append((i.name, i.parent, i.stem, i.suffix[1:],                                  time.ctime(i.stat().st_ctime), i.name.lower()))        columns = ["name", "parent", "stem", "suffix", "created", "search"]        pd.set_option('display.max_colwidth', -1)        df = pd.DataFrame.from_records(all_files, columns=columns)        if find_name is None:            return df        if ignorecase is True:            return df[df['search'].str.contains(find_name.lower())]        return df[df['name'].str.contains(find_name)]    @staticmethod    def massive_data_sampler(filename, chunk_sample=100, sample_limit=0, chunk_size=100000, **kwargs) -> pd.DataFrame:        """ takes a sample set of data from massive files that can't be loaded into memory        :param filename: the name of the csv file to be loaded        :param chunk_sample: the sample size from each chunk, if 0 then same as chunk_size        :param sample_limit: the the sample limit. if 0 then no limit        :param chunk_size: the size of the chunks to take from the file        :param kwargs: Additional kwargs to be added to the csv read such as 'sep='        :return: the sample pd.DataFrame        """        if chunk_size is None or chunk_size < 1:            chunk_size = 100000        if chunk_sample is None or chunk_sample == 0 or chunk_size < chunk_sample:            chunk_sample = chunk_size        df = None        _row_count = 0        for chunk in pd.read_csv(filename, chunksize=chunk_size, **kwargs):            if chunk.index.max() <= chunk.index.min():                break            _sample_length = chunk_size - chunk_sample            _index_length = chunk.index.max() - chunk.index.min()            if _sample_length > _index_length:                _sample_length = _index_length            _index_list = sorted(random.sample(range(chunk.index.min(), chunk.index.max()), k=_sample_length))            chunk_drop = chunk.drop(axis=0, index=_index_list)            if df is None:                df = chunk_drop            else:                df = df.append(chunk_drop)            _row_count += chunk_sample            if _row_count >= sample_limit > 0:                break        return df    @staticmethod    def train_test_sampler(df, train_fraction=None, random_state=None) -> list:        """ returns a training and testing pandas.Dataframe from the passed one        :param df: the pandas.Dataframe to take the sampler from        :param train_fraction: the precentage fraction of the train model. Default 0.75        :param random_state: a randomn state value. Default 0 < n < 100        :return: a train df and a test df        """        if train_fraction is None:            train_fraction = 0.75        if random_state is None:            random_state = np.random.randint(1, 100)        train = df.sample(frac=train_fraction, random_state=random_state)        test = df.loc[~df.index.isin(train.index), :]        return [train, test]    @staticmethod    def create_dictionary(df, contract_name, file_out) -> None:        """ creates an excel spreadsheet of the data dictionary        :param df: the DataFrame to base the dictionary on        :param contract_name: the contract name under which the configuration is stored        :param file_out: the name of the excel file        :return:        """        path = file_out        with closing(pd.ExcelWriter(path, engine='xlsxwriter')) as writer:            # data dictionary            sheet = contract_name            df_dictionary = DataDiscovery.data_dictionary(df, stylise=False)            df_dictionary.to_excel(writer, sheet_name=sheet, startrow=1, index=False, header=False)            DataDiscovery._dictionary_format(df_dictionary, writer, sheet)            # stat disctionary            sheet = '{}_stat'.format(contract_name)            df_stats = DataDiscovery.stat_dictionary(df).reset_index().rename(columns={'index': ''})            df_stats.to_excel(writer, sheet_name=sheet, startrow=1, index=False, header=False)            DataDiscovery._dictionary_format(df_stats, writer, sheet)            # category dictionary            sheet = '{}_cat'.format(contract_name)            col = 0            for df_category in DataDiscovery._category_dictionary(df):                df_category.to_excel(writer, sheet_name=sheet, startcol=col, index=False, header=True)                col += 3                DataDiscovery._dictionary_format(None, writer, sheet)            writer.save()        return    # @staticmethod    # def report_analysis(df, associate: list=None, stylise: bool=False):    #     """    #    #     :param df:    #     :param associate:    #     :return:    #     """    #     stylise = True if not isinstance(stylise, bool) else stylise    #     style = [{'selector': 'th', 'props': [('font-size', "120%"), ("text-align", "center")]},    #              {'selector': '.row_heading, .blank', 'props': [('display', 'none;')]}]    #     pd.set_option('max_colwidth', 75)    #     df_len = len(df)    #     file = []    #     labels = ['Attribute', 'dType', '%_Null', '%_Dom', '%_Nxt', 'Count', 'Unique', 'Observations']    #     for c in associate:    #         # TODO: Complete the analysis report    #         pass    @staticmethod    def analysis_dictionary(df, granularity: int=None, col_kwargs: dict=None) -> pd.DataFrame:        """ creates an analysis dictionary that shows an overview of the flat distribution and        weighting of the attributes        :param df: the dataframe to analyse        :param granularity: the granularity to be applied across the df attributes        :param col_kwargs: a column kwargs to specify the analysis of specific attributes                 {'attrA': {'granularity': 6}        :return: an analysis data dictionary        """        tools = DataDiscovery        pd.set_option('max_colwidth', 100)        df_len = len(df)        file = []        labels = ['Attribute', 'Type', 'Unique', '% Nulls', 'Sample', 'Selection', 'Weighting',                  'Granularity', 'Lower', 'Upper']        for c in df.columns.sort_values().values:            result = None            kwargs = {}            if isinstance(col_kwargs, dict) and c in col_kwargs.keys():                kwargs = col_kwargs.get(c)            line = [c,                    str(df[c].dtype),                    df[c].nunique()]            if df[c].dtype.name == 'category'or df[c].dtype.name.startswith('bool'):                result = tools.analyse_category(df[c], **kwargs)            elif df[c].dtype.name.startswith('int') or df[c].dtype.name.startswith('float'):                if isinstance(granularity, int) and 'granularity' not in kwargs.keys():                    kwargs['granularity'] = granularity                result = tools.analyse_number(df[c], **kwargs)            elif df[c].dtype.name.startswith('date'):                if isinstance(granularity, int) and 'granularity' not in kwargs.keys():                    kwargs['granularity'] = granularity                result = tools.analyse_date(df[c], **kwargs)            if result is not None:                line.append(result.get('null_values')[0])                line.append(result.get('sample')[0])                line.append(result.get('selection'))                line.append(result.get('weighting'))                if result.get('granularity') is not None:                    line.append(result.get('granularity'))                    line.append(result.get('lower'))                    line.append(result.get('upper'))                else:                    line.append(len(result.get('selection')))                    line += ['', '']            else:                line += [round(df[c].isnull().sum() / df_len, 2), df_len, '', '', '', '', '']            file.append(line)        return pd.DataFrame(file, columns=labels)    @staticmethod    def data_dictionary(df, stylise: bool=False, inc_next_dom: bool=False, report_header: str=None,                        condition: str=None) -> pd.DataFrame:        """ returns a DataFrame of a data dictionary showing 'Attribute', 'Type', '% Nulls', 'Count',        'Unique', 'Observations' where attribute is the column names in the df        Note that the subject_matter, if used, should be in the form:            { subject_ref, { column_name : text_str}}        the subject reference will be the header of the column and the text_str put in next to each attribute row        :param df: (optional) the pandas.DataFrame to get the dictionary from        :param stylise: (optional) returns a stylised dataframe with formatting        :param inc_next_dom: (optional) if to include the next dominate element column        :param report_header: (optional) filter on a header where the condition is true. Condition must exist        :param condition: (optional) the condition to apply to the header. Header must exist. examples:                ' > 0.95', ".str.contains('shed')"        :return: a pandas.Dataframe        """        stylise = True if not isinstance(stylise, bool) else stylise        inc_next_dom = False if not isinstance(inc_next_dom, bool) else inc_next_dom        style = [{'selector': 'th', 'props': [('font-size', "120%"), ("text-align", "center")]},                 {'selector': '.row_heading, .blank', 'props': [('display', 'none;')]}]        pd.set_option('max_colwidth', 75)        df_len = len(df)        file = []        labels = ['Attribute', 'dType', '%_Null', '%_Dom', '%_Nxt', 'Count', 'Unique', 'Observations']        for c in df.columns.sort_values().values:            line = [c,                    str(df[c].dtype),                    round(df[c].replace('', np.nan).isnull().sum() / df_len, 3)]            # Predominant Difference            nulls_list = ['NaN', 'nan', 'null', '', 'None']            col = deepcopy(df[c])            col.replace(nulls_list, np.nan, inplace=True)            if len(col.dropna()) > 0:                result = (col.value_counts() / np.float(len(col.dropna()))).sort_values(ascending=False).values                line.append(round(result[0], 3))                if len(result) > 1:                    line.append(round(result[1], 3))                else:                    line.append(0)            else:                line.append(0)                line.append(0)            # value count            line.append(df[c].notnull().sum())            # unique            line.append(df[c].nunique())            # Observations            if df[c].dtype.name == 'category':                line_str = str('|'.join(df[c].cat.categories.astype(str)))                line.append('{}...'.format(line_str[:100]) if len(line_str) > 100 else line_str)            elif df[c].dtype.name == 'bool':                line.append(str(' | '.join(df[c].map({True: 'True', False: 'False'}).unique())))            elif df[c].dtype.name == 'object':                value_set = df.loc[df[c].notnull(), c].drop_duplicates().astype(str)                if len(value_set) > 0:                    sample_num = 3 if len(value_set) >= 3 else len(value_set)                    sample = str(' | '.join(value_set.sample(n=sample_num).values))                else:                    sample = 'Null Values'                line_str = 'Sample: {}'.format(sample)                line.append('{}...'.format(line_str[:100]) if len(line_str) > 100 else line_str)            elif df[c].dtype.name.startswith('int') \                    or df[c].dtype.name.startswith('float') \                    or df[c].dtype.name.startswith('date'):                my_str = 'max=' + str(df[c].max()) + ' | min=' + str(df[c].min())                if df[c].dtype.name.startswith('date'):                    my_str += ' | yr mean= ' + str(round(df[c].dt.year.mean(), 0)).partition('.')[0]                else:                    my_str += ' | mean=' + str(round(df[c].mean(), 2))                line.append(my_str)            else:                line.append('')            file.append(line)        df_dd = pd.DataFrame(file, columns=labels)        if isinstance(report_header, str) and report_header in labels and isinstance(condition, str):            str_value = "df_dd['{}']{}".format(report_header, condition)            try:                df_dd = df_dd.where(eval(str_value)).dropna()            except(SyntaxError, ValueError):                pass        if stylise:            df_style = df_dd.style.set_table_styles(style)            _ = df_style.applymap(DataDiscovery._highlight_null_dom, subset=['%_Null', '%_Dom'])            _ = df_style.applymap(lambda x: 'color: white' if x > 0.98 else 'color: black', subset=['%_Null', '%_Dom'])            _ = df_style.applymap(DataDiscovery._highlight_next, subset=['%_Nxt'])            _ = df_style.applymap(lambda x: 'color: white' if x < 0.02 else 'color: black', subset=['%_Nxt'])            _ = df_style.applymap(DataDiscovery._dtype_color, subset=['dType'])            _ = df_style.applymap(DataDiscovery._color_unique, subset=['Unique'])            _ = df_style.applymap(lambda x: 'color: white' if x < 2 else 'color: black', subset=['Unique'])            _ = df_style.format({'%_Null': "{:.1%}", '%_Dom': '{:.1%}', '%_Nxt': '{:.1%}'})            _ = df_style.set_caption('%_Dom: The % most domninant element - %_Nxt: The % next most dominant element')            _ = df_style.set_properties(subset=['Attribute'],  **{'font-weight': 'bold', 'font-size': "120%"})            if not inc_next_dom:                df_style.hide_columns('%_Nxt')                _ = df_style.set_caption('%_Dom: The % most domninant element')            return df_style        if not inc_next_dom:            df_dd.drop('%_Nxt', axis='columns', inplace=True)        return df_dd    @staticmethod    def _category_dictionary(df) -> list:        rtn_list = []        for c in Cleaner.filter_columns(df, dtype=['category']):            df_stat = df[c].value_counts().reset_index()            df_stat.columns = [c, 'Count']            rtn_list.append(df_stat)        return rtn_list    @staticmethod    def stat_dictionary(df) -> pd.DataFrame:        """ Creates a statistics dictionary for all number fields        :param df: the pandas.DataFrame to get the dictionary from        :return: a pandas.Dataframe        """        file = []        for c in Cleaner.filter_columns(df, dtype=['float_', 'int_']):            precision = 2 if df[c].dtype == 'float_' else 0            describe = df[c].describe()            line = [c,                    df[c].dtype.name,                    int(describe.iloc[0]),                    df[c].nunique(),                    round(df[c].mean(), precision),                    round(df[c].median(), precision),                    round(df[c].var(ddof=0), precision),                    round(df[c].std(ddof=0), precision),                    round(df[c].min(), precision),                    round(describe.iloc[4], precision),                    round(describe.iloc[5], precision),                    round(describe.iloc[6], precision),                    round(df[c].max(), precision)]            file.append(line)        labels = ['', 'dtype', 'count', 'unique', 'mean', 'median', 'var', 'std', 'min', '25%', '50%', '75%', 'max']        df_stat = pd.DataFrame(file, columns=labels).transpose()        # reset the headers        df_stat.columns = df_stat.iloc[0]        return df_stat[1:]    @staticmethod    def _dictionary_format(df, writer, sheet):        # First set the workbook up        workbook = writer.book        number_fmt = workbook.add_format({'num_format': '#,##0', 'align': 'right', 'valign': 'top'})        decimal_fmt = workbook.add_format({'num_format': '#,##0.00', 'align': 'right', 'valign': 'top'})        percent_fmt = workbook.add_format({'num_format': '0%', 'align': 'right', 'valign': 'top'})        attr_format = workbook.add_format({'bold': True, 'align': 'left', 'text_wrap': True, 'valign': 'top'})        text_fmt = workbook.add_format({'align': 'left', 'text_wrap': True, 'valign': 'top'})        header_format = workbook.add_format({'bold': True, 'text_wrap': True, 'valign': 'top', 'font_size': '12',                                             'fg_color': '#8B0000', 'font_color': '#FFFFFF', 'border': 1})        # The the worksheets        worksheet = writer.sheets[sheet]        worksheet.set_zoom(100)        if sheet.endswith('_stat'):            worksheet.set_column('A:A', 15, attr_format)            letter_count = 0            pre_letter_count = 0            pre_letter = ''            for index in range(1, len(df.columns)):                letter = chr(letter_count + 65)  # A-Z => 65-91                column = df.columns[index]                width, _ = DataDiscovery.col_width(df, column)                fmt = decimal_fmt if df[column].iloc[0].startswith('float') else number_fmt                worksheet.set_column('{pre}{letter}:{pre}{letter}'.format(pre=pre_letter, letter=letter),                                     width + 4, fmt)                letter_count += 1                if letter == 'Z':                    pre_letter = chr(65 + pre_letter_count)                    pre_letter_count += 1                    letter_count = 0        elif sheet.endswith('_cat'):            for letter in range(65, 91, 3):                worksheet.set_column('{}:{}'.format(chr(letter), chr(letter)), 20, text_fmt)                worksheet.set_column('{}:{}'.format(chr(letter + 1), chr(letter + 1)), 8, number_fmt)                worksheet.conditional_format('{}1:{}40'.format(chr(letter + 1), chr(letter + 1)),                                             {'type': '3_color_scale', 'min_color': "#ecf9ec",                                              'mid_color': "#c6ecc6", 'max_color': "#8cd98c"})        else:            worksheet.set_column('A:A', 20, attr_format)            worksheet.set_column('B:B', 12, text_fmt)            worksheet.set_column('C:C', 8, percent_fmt)            worksheet.set_column('D:E', 10, number_fmt)            worksheet.set_column('F:G', 80, text_fmt)        if not sheet.endswith('_cat'):            for col_num, value in enumerate(df.columns.values):                worksheet.write(0, col_num, value, header_format)        return    @staticmethod    def col_width(df, column, as_value=False) -> tuple:        """ gets the max and min length or values of a column as a (max, min) tuple        :param df: the pandas.DataFrame        :param column: the column to find the max and min for        :param as_value: if the return should be a length or the values. Default is False        :return: returns a tuple with the (max, min) length or values        """        if as_value:            field_length = df[column].astype(str).str.len()            return df.loc[field_length.argmax(), column], df.loc[field_length.argmin(), column]        return df[column].astype(str).str.len().max(), df[column].astype(str).str.len().min()    @staticmethod    def _dtype_color(dtype: str):        """Apply color to types"""        if str(dtype).startswith('cat'):            color = '#208a0f'        elif str(dtype).startswith('int'):            color = '#0f398a'        elif str(dtype).startswith('float'):            color = '#2f0f8a'        elif str(dtype).startswith('date'):            color = '#790f8a'        elif str(dtype).startswith('bool'):            color = '#08488e'        else:            return ''        return 'color: %s' % color    @staticmethod    def _highlight_null_dom(x: str):        x = float(x)        if not isinstance(x, float) or x < 0.65:            return ''        elif x < 0.85:            color = '#ffede5'        elif x < 0.90:            color = '#fdcdb9'        elif x < 0.95:            color = '#fcb499'        elif x < 0.98:            color = '#fc9576'        elif x < 0.99:            color = '#fb7858'        elif x < 0.997:            color = '#f7593f'        else:            color = '#ec382b'        return 'background-color: %s' % color    @staticmethod    def _highlight_next(x: str):        x = float(x)        if not isinstance(x, float):            return ''        elif x < 0.01:            color = '#ec382b'        elif x < 0.02:            color = '#f7593f'        elif x < 0.03:            color = '#fb7858'        elif x < 0.05:            color = '#fc9576'        elif x < 0.08:            color = '#fcb499'        elif x < 0.12:            color = '#fdcdb9'        elif x < 0.18:            color = '#ffede5'        else:            return ''        return 'background-color: %s' % color    @staticmethod    def _color_unique(x: str):        x = int(x)        if not isinstance(x, int):            return ''        elif x < 2:            color = '#ec382b'        elif x < 3:            color = '#a1cbe2'        elif x < 5:            color = '#84cc83'        elif x < 10:            color = '#a4da9e'        elif x < 20:            color = '#c1e6ba'        elif x < 50:            color = '#e5f5e0'        elif x < 100:            color = '#f0f9ed'        else:            return ''        return 'background-color: %s' % color