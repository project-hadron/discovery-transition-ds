Introduction
============

Though synthetic data has been a topic of conversation for many years, With the emergence of data
science and its exponential growth we have seen an equal demand in the need for synthetic data.
Gartner predicted by 2030 most of the data used in AI will be artificially generated by rules,
statistical models, simulations or other techniques. MIT Tech Review has called synthetic data
one of the top breakthrough technologies of 2022.

Synthetic data is representative data that, depending on its application, holds statistical and
distributive characteristics of its real world counterpart.

The end goal with synthetic data generation is to take sample data sources along with the broader
knowledge of subject matter experts and create a synthetic data source with similar statistical
properties. Having similar statistical properties means that we need to reproduce the distribution
to the extent that we should ultimately be able to infer the same conclusion from both versions of
the data.

With organizations becoming increasingly aware of the value of data, their inclination is to heavily
control its accessibility which leaves data hungry artificial intelligence teams without their core
resource. Even without this restriction, collecting, labeling, training, and deploying data is
difficult, costly, and time-consuming—multiple surveys have uncovered that data science teams spend
anywhere from 50–80% of their time collecting and cleaning data. Additionally real-world data raises
ethical and privacy concerns along with the potential of unpredictable bias.

One should be cautious as synthetic data is a collective term, and not all synthetic data has the
same characteristics. Synthetic datasets are not simply a re-design of previously existing data but
are a set of completely new data points, Synthetic data can be defined by the amount of effort and
knowledge put into it.